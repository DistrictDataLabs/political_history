{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import graphlab\n",
    "from graphlab import SFrame\n",
    "from nltk.collocations import *       #Bad form\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import *\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "LOCAL_DATA_PATH = 'C:\\Users\\Kevin OGallath34/political_history/data'\n",
    "LOCAL_SAVE_PATH = 'C:\\Users\\Kevin OGallath34/political_history/processed_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in Candidates_df pickle file\n",
    "candidates_df = pd.read_pickle('C:\\Users\\Kevin OGallath34\\political_history\\processed_data\\candidates_df1.pkl')\n",
    "#print test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "3       Republican\n",
       "6       Republican\n",
       "7       Republican\n",
       "8       Republican\n",
       "9       Republican\n",
       "10        Democrat\n",
       "11      Republican\n",
       "12      Republican\n",
       "14      Republican\n",
       "15      Republican\n",
       "17        Democrat\n",
       "18        Democrat\n",
       "19        Democrat\n",
       "20        Democrat\n",
       "21        Democrat\n",
       "22        Democrat\n",
       "25        Democrat\n",
       "27        Democrat\n",
       "30      Republican\n",
       "32      Republican\n",
       "33        Democrat\n",
       "34      Republican\n",
       "36        Democrat\n",
       "37      Republican\n",
       "38        Democrat\n",
       "39      Republican\n",
       "40      Republican\n",
       "41      Republican\n",
       "42      Republican\n",
       "44      Republican\n",
       "           ...    \n",
       "8023      Democrat\n",
       "8024      Democrat\n",
       "8026      Democrat\n",
       "8027    Republican\n",
       "8028      Democrat\n",
       "8029    Republican\n",
       "8030    Republican\n",
       "8031    Republican\n",
       "8032    Republican\n",
       "8041      Democrat\n",
       "8042      Democrat\n",
       "8043      Democrat\n",
       "8045      Democrat\n",
       "8046      Democrat\n",
       "8048      Democrat\n",
       "8049      Democrat\n",
       "8050      Democrat\n",
       "8051      Democrat\n",
       "8052      Democrat\n",
       "8053      Democrat\n",
       "8054      Democrat\n",
       "8057    Republican\n",
       "8059    Republican\n",
       "8060    Republican\n",
       "8061    Republican\n",
       "8063    Republican\n",
       "8065    Republican\n",
       "8066    Republican\n",
       "8067    Republican\n",
       "8068    Republican\n",
       "Name: affiliation, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_df['affiliation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label data - 0 for democrat, 1 for republican\n",
    "candidates_df['affiliation'].replace({'Democrat':0, 'Republican':1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new lists just with labels and text\n",
    "labels = candidates_df['affiliation']\n",
    "paragraph_text = candidates_df['text2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sentiment Analysis using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "from textblob_aptagger import PerceptronTagger\n",
    "from textblob import Blobber\n",
    "\n",
    "tb2 = Blobber(pos_tagger=PerceptronTagger())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Polarity of Subjective Sentences in Text (>0.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Load text2 into TextBlobber\n",
    "#  textblob.sentences    parses text into each sentence\n",
    "#  sentence.sentiment.subjectivity     Sentiment value for the sentence\n",
    "#  sentence.sentiment.polarity          Polarity value for the sentence\n",
    "\n",
    "def clean_text(text):\n",
    "    tbs = tb2(text)\n",
    "    pol = 0.0\n",
    "    for sentence in tbs.sentences:\n",
    "             sub = sentence.sentiment.subjectivity             # returns (polarity, subjectivity)\n",
    "             if sub > 0.49:\n",
    "                    pol1 = sentence.sentiment.polarity\n",
    "                    pol = pol + pol1\n",
    "    return(pol) \n",
    "\n",
    "# iterates through each Text field calculating the polarity value for the subjective sentences only\n",
    "\n",
    "for _, text2 in candidates_df.iteritems():\n",
    "     candidates_df['polarity'] =  candidates_df['text2'].apply(lambda row: clean_text(row))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Polarity of All Sentences in Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Load text2 into TextBlobber\n",
    "#  textblob.sentences    parses text into each sentence\n",
    "#  sentence.sentiment.subjectivity     Sentiment value for the sentence\n",
    "#  sentence.sentiment.polarity          Polarity value for the sentence\n",
    "\n",
    "def clean_text2(text):\n",
    "    tbs = tb2(text)\n",
    "    pol = 0.0\n",
    "    for sentence in tbs.sentences:\n",
    "             pol1 = sentence.sentiment.polarity             # returns (polarity, subjectivity)\n",
    "             pol = pol + pol1\n",
    "    return(pol) \n",
    "\n",
    "# iterates through each Text field calculating the polarity value for the subjective sentences only\n",
    "for _, text2 in candidates_df.iteritems():\n",
    "     candidates_df['polarity_all'] =  candidates_df['text2'].apply(lambda row: clean_text2(row))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Finish cleaning up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "from textblob_aptagger import PerceptronTagger\n",
    "from textblob.taggers import NLTKTagger\n",
    "from textblob import Blobber\n",
    "\n",
    "nltk_tagger = NLTKTagger()\n",
    "tb2 = Blobber(pos_tagger=nltk_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordstring = []\n",
    "def clean_tokens (text):\n",
    "    tbs = tb2(text)\n",
    "    words_para = tbs.words \n",
    "    return(words_para)\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    \n",
    "for _, text2 in candidates_df.iteritems():\n",
    "    candidates_df['words_para'] =  candidates_df['text2'].apply(lambda row: clean_tokens(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'textblob.blob.WordList'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4766e3269aea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwordstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_para\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mwords_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcandidates_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_para'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwordstring\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mwords_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kevin OGallath34\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2235\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2237\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2239\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas\\lib.c:63043)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-4766e3269aea>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwordstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_para\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mwords_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcandidates_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_para'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwordstring\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mwords_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0220e7d17fca>\u001b[0m in \u001b[0;36mclean_tokens\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwordstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_tokens\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mwords_para\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_para\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kevin OGallath34\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\textblob\\blob.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    738\u001b[0m                         \u001b[0mnp_extractor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_extractor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m                         \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                         classifier=self.classifier)\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kevin OGallath34\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\textblob\\blob.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             raise TypeError('The `text` argument passed to `__init__(text)` '\n\u001b[1;32m--> 344\u001b[1;33m                             'must be a string, not {0}'.format(type(text)))\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclean_html\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             raise NotImplementedError(\"clean_html has been deprecated. \"\n",
      "\u001b[1;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'textblob.blob.WordList'>"
     ]
    }
   ],
   "source": [
    "wordstring = []\n",
    "for _, words_para in candidates_df.iteritems():\n",
    "        words_list = candidates_df['words_para']\n",
    "\n",
    "wordstring += words_list\n",
    "word_all = wordstring.split()\n",
    "word_freq = [word_all.count(w) for w in word_all] # a list comprehension word_tot = zip(word_all, word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_stems(text):\n",
    "    tbs = tb2(text)\n",
    "    tokens = tbs.tags                                  \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    return(tokens)\n",
    "                     \n",
    "for _, text2 in candidates_df.iteritems():\n",
    "     candidates_df['tokens'] =  candidates_df['text2'].apply(lambda row: clean_stems(row)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alternative creation of tokens (compare results )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse into sentences and tokenize into POS\n",
    "def tag_pos(text):\n",
    "    pos_tags=[]\n",
    "    # first tokenize by sentence\n",
    "    for sent in nltk.sent_tokenize(text): \n",
    "        tags   = nltk.pos_tag(sent)\n",
    "        pos_tags = tags.append(tags)\n",
    "        return(pos_tags)\n",
    "# iterates through each Text field calculating the polarity value for the subjective sentences only\n",
    "for _, text2 in candidates_df.iteritems():\n",
    "     candidates_df['pos_tags'] =  candidates_df['text2'].apply(lambda row: tag_pos(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter POS Tokens to only Noun, Verb, Adjective, and Adverbs\n",
    "for _, pos_tags in candidates_df.iteritems():\n",
    "    candidates_df['pos_filter'] = candidates_df['pos_tags'].apply(lambda x: [(t[0],t[1]) for t in x if t[1]=='VB' or t[1]=='VBD' or t[1]=='VBG' or t[1]=='VBN' or t[1]=='VBP' or t[1]=='VBZ' or t[1]=='NN' or t[1]=='NNS' or t[1]=='NNP' or t[1]=='NNPS'\n",
    "                                          or t[1]=='JJ' or t[1]=='JJR' or t[1]=='JJS' or t[1]=='RB' or t[1]=='RBR' or t[1]=='RBS']) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframe as pickle (pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export pos_df at this point for recovery\n",
    "directory_name = LOCAL_SAVE_PATH\n",
    "base_filename = 'pos_df'\n",
    "suffix = '.pkl'\n",
    "save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "candidates_df.to_pickle(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import SFrame\n",
    "sf = sframe.Sframe(data=candidates_df)\n",
    "SFrame.show(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_final = candidates_df.loc[:,'date','speaker','affiliation','text2','pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export pos_df at this point for recovery\n",
    "directory_name = LOCAL_SAVE_PATH\n",
    "base_filename = 'pos_final'\n",
    "suffix = '.pkl'\n",
    "save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "candidates_df.to_pickle(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Goverment vs Opposition impact on emotional polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_df = candidates_df.loc[:,['affiliation','polarity','polarity_all']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = sum_df.groupby('affiliation').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "candidates_df['date'] = pd.to_datetime(candidates_df['date'])\n",
    "candidates_df['year'] = candidates_df['date'].dt.year \n",
    "        \n",
    "candidates_df['year']=candidates_df['year'].astype(str)   \n",
    "\n",
    "#party of governing president when the debates occurred.    0 = democratic    1 = republican\n",
    "presidents_dict = { \n",
    "\"2000\": 0,\n",
    "\"2001\": 1,\n",
    "\"2002\": 1,\n",
    "\"2003\": 1,\n",
    "\"2004\": 1,\n",
    "\"2005\": 1,\n",
    "\"2006\": 1,\n",
    "\"2007\": 1,\n",
    "\"2008\": 1, \n",
    "\"2009\": 0,\n",
    "\"2010\": 0,\n",
    "\"2011\": 0,\n",
    "\"2012\": 0,\n",
    "\"2013\": 0,\n",
    "\"2014\": 0,\n",
    "\"2015\": 0,\n",
    "\"2016\": 0}\n",
    "  \n",
    "    \n",
    "\n",
    "def expand_presidents(search_year):\n",
    "     for year, opposition in presidents_dict.items():\n",
    "        \n",
    "        if year == search_year:\n",
    "            return opposition\n",
    "\n",
    "\n",
    "for _, year in candidates_df.iteritems():\n",
    "     candidates_df['opposition'] =  candidates_df['year'].apply(lambda row: expand_presidents(row))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "candidates_df['opposition']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Polarity totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_df = candidates_df.loc[:,['affiliation','polarity','polarity_all','opposition']\n",
    "grouped = sum_df.groupby('affiliation','opposition').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "from graphlab import SFrame\n",
    "graphlab.canvas.set_target('ipynb')\n",
    "sf = SFrame(data=candidates_df)\n",
    "sf.show(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple demo of a scatter plot.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "x = grouped\n",
    "y = grouped\n",
    "colors = np.random.rand(N)\n",
    "area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radiuses\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
    "plt.show()\n",
    "x = np.random.rand(N)\n",
    "y = np.random.rand(N)\n",
    "colors = np.random.rand(N)\n",
    "area = np.pi * (15 * np.random.rand(N))**2  # 0 to 15 point radiuses\n",
    "\n",
    "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "candidates_df['affiliation'] == 1 \n",
    "candidates_df['opposition'] == 1\n",
    "colors = np.where(candidates_df['opposition'] == 1, 'k', 'r')\n",
    "plt.scatter(candidates_df['opposition'], candidates_df['polarity'], s=120, c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates_df['affiliation'] == 0 \n",
    "candidates_df['opposition'] == 0\n",
    "colors = np.where(candidates_df['opposition'] == 0, 'k', 'r')\n",
    "plt.scatter(candidates_df['opposition'], candidates_df['polarity'], s=120, c=colors)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
