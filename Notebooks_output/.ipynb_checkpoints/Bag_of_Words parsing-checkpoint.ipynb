{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### munging/parsing/processing json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ideas:  \n",
    "use date of debate as another feature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "LOCAL_DATA_PATH = 'C:\\Users\\JoAnna\\political_history\\data'\n",
    "LOCAL_SAVE_PATH = 'C:\\Users\\JoAnna\\political_history\\processed_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general processing for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import all json files, concatenate into pandas dataframe\n",
    "all_files = glob.glob(LOCAL_DATA_PATH + '/*.json')\n",
    "\n",
    "df = pd.concat((pd.read_json(f, orient='index') for f in all_files))\n",
    "\n",
    "#concatenating resulted in non-unique index, re-index\n",
    "#df.index.is_unique\n",
    "\n",
    "df['index'] = np.arange(len(df))\n",
    "df = df.set_index('index')\n",
    "\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from speaker: strip spaces and special characters, make lowercase\n",
    "df.speaker = df.speaker.str.strip().str.lower().str.replace(' ','').str.replace('.', '').str.replace(':','')\n",
    "\n",
    "#get list of unique values for speaker\n",
    "unique_speaker = pd.unique(df.speaker.ravel())\n",
    "#print len(unique_speaker)\n",
    "#print unique_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#map list of candidates to political party\n",
    "speaker_to_party = {'trump': 'Republican',\n",
    "                    'clinton': 'Democrat',\n",
    "                    'pence': 'Republican',\n",
    "                    'kaine': 'Democrat',\n",
    "                    'republicanpresidentialnomineewmittromney': 'Republican',\n",
    "                    'govromney': 'Republican',\n",
    "                    'thepresident\\u2014': 'Republican',\n",
    "                    'govromney\\u2014': 'Republican',\n",
    "                    'govromney\\u2014\\u2014': 'Republican',\n",
    "                    'thepresident': 'Democrat',\n",
    "                    'representativepaulryan': 'Republican',\n",
    "                    'ryan': 'Republican',\n",
    "                    'vicepresidentjosephbiden': 'Democrat',\n",
    "                    'biden': 'Democrat',\n",
    "                    'mccain': 'Republican',\n",
    "                    'obama': 'Democrat',\n",
    "                    'palin': 'Republican',\n",
    "                    'presidentbush': 'Republican',\n",
    "                    'senatorjohnfkerry': 'Democrat',\n",
    "                    'senatorkerry': 'Democrat',\n",
    "                    'cheney': 'Republican',\n",
    "                    'edwards': 'Democrat',\n",
    "                    'bush': 'Republican',\n",
    "                    'gore': 'Democrat',\n",
    "                    'lieberman': 'Democrat'}\n",
    "\n",
    "#make new column in dataframe for affiliation\n",
    "df['affiliation'] = df['speaker']\n",
    "df['affiliation'].replace(speaker_to_party, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create two new dataframes, one for republicans, one for democrats\n",
    "republican_df = df.loc[df['affiliation'] == 'Republican']\n",
    "democrat_df = df.loc[df['affiliation'] == 'Democrat']\n",
    "\n",
    "#create combined data frame - better for train/test split (sort of...)\n",
    "candidates_df = df.loc[df['affiliation'].isin(['Republican','Democrat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export new dataframe for others to use\n",
    "directory_name = LOCAL_SAVE_PATH\n",
    "base_filename = 'candidates'\n",
    "suffix = '.pkl'\n",
    "save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "candidates_df.to_pickle(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read pickled data\n",
    "#test_df = pd.read_pickle('C:\\Users\\JoAnna\\political_history\\processed_data\\candidates.pkl')\n",
    "#print test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoAnna\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.py:3443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "#label data - 0 for democrat, 1 for republican\n",
    "candidates_df['affiliation'].replace({'Democrat':0, 'Republican':1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the text column for bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new lists just with labels and text\n",
    "labels = candidates_df['affiliation']\n",
    "paragraph_text = candidates_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define function to tokenize and stem\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, removes stop words, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token if word not in stops]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) \n",
    "\n",
    "num_paragraphs= len(paragraph_text)\n",
    "cleaned_paragraphs = []\n",
    "#loop over paragraph_text to clean\n",
    "for paragraph in paragraph_text:\n",
    "    cleaned_text = clean_text(paragraph)\n",
    "    cleaned_paragraphs.append(cleaned_text)\n",
    "\n",
    "#print len(cleaned_paragraphs)\n",
    "#print cleaned_paragraphs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export labels and cleaned paragraphs\n",
    "os.chdir(LOCAL_SAVE_PATH)\n",
    "pickle.dump(labels, open(\"bow_labels.pkl\", \"w\"))\n",
    "pickle.dump(cleaned_paragraphs, open(\"bow_processed_text.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define new function to tokenize and stem, keep stopwords\n",
    "def clean_text_nostop(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    #stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) \n",
    "\n",
    "num_paragraphs= len(paragraph_text)\n",
    "cleaned_paragraphs_nostop = []\n",
    "#loop over paragraph_text to clean\n",
    "for paragraph in paragraph_text:\n",
    "    cleaned_text_nostop = clean_text_nostop(paragraph)\n",
    "    cleaned_paragraphs_nostop.append(cleaned_text_nostop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export labels and cleaned paragraphs, no stop words\n",
    "os.chdir(LOCAL_SAVE_PATH)\n",
    "#pickle.dump(labels, open(\"bow_labels.pkl\", \"w\"))\n",
    "pickle.dump(cleaned_paragraphs_nostop, open(\"bow_processed_text_nostop.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize, Stem text and make each paragraph an ordered list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_list(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    #stemmed_text = \" \".join(stems)\n",
    "    return stems\n",
    "\n",
    "num_paragraphs= len(paragraph_text)\n",
    "cleaned_paragraphs_list = []\n",
    "#loop over paragraph_text to clean\n",
    "for paragraph in paragraph_text:\n",
    "    cleaned_text_list = clean_text_list(paragraph)\n",
    "    cleaned_paragraphs_list.append(cleaned_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'let', u'me', u'mention', u'anoth', u'regul', u'in', u'dodd', u'frank', u'you', u'say', u'we', u'were', u'give', u'mortgag', u'to', u'peopl', u'who', u'weren', u't', u'qualifi', u'that', u's', u'exact', u'right', u'it', u's', u'one', u'of', u'the', u'reason', u'for', u'the', u'great', u'financi', u'calam', u'we', u'had', u'and', u'so', u'dodd', u'frank', u'correct', u'say', u'we', u'need', u'to', u'have', u'qualifi', u'mortgag', u'and', u'if', u'you', u'give', u'a', u'mortgag', u'that', u's', u'not', u'qualifi', u'there', u'are', u'big', u'penalti', u'except', u'they', u'didn', u't', u'ever', u'go', u'on', u'to', u'defin', u'what', u'a', u'qualifi', u'mortgag', u'was']\n"
     ]
    }
   ],
   "source": [
    "#print cleaned_paragraphs_list[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export labels and cleaned paragraphs, no stop words, text in a list\n",
    "os.chdir(LOCAL_SAVE_PATH)\n",
    "#pickle.dump(labels, open(\"bow_labels.pkl\", \"w\"))\n",
    "pickle.dump(cleaned_paragraphs_list, open(\"bow_processed_text_list.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just export text without tokenizing and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paragraph processing for word2vec\n",
    "#https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "\n",
    "#start with paragraph text\n",
    "\n",
    "def paragraph_to_wordlist(paragraph, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    paragraph_clean = re.sub(\"[^a-zA-Z]\",\" \", paragraph)\n",
    "    words = paragraph_clean.lower().split()\n",
    "    #Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#split paragraph into sentences, then sentences into words\n",
    "\n",
    "import nltk.data\n",
    "nltk.download()\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Define a function to split a review into parsed sentences\n",
    "def paragraph_to_sentences( paragraph, tokenizer, remove_stopwords=False ):\n",
    "    raw_sentences = tokenizer.tokenize(paragraph.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(paragraph_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "18711\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for paragraph in paragraph_text:\n",
    "    sentences += paragraph_to_sentences(paragraph, tokenizer)\n",
    "    \n",
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'well', u'good']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this processing does not keep the same index for the sentences and labels from the paragraph. Fix this, or don't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
