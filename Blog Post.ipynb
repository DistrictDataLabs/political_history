{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Shibboleth:\n",
    "NLP and the presidential debates</h1>\n",
    "<em>\n",
    "Jo Anna Capp\n",
    "Kevin O’Gallagher\n",
    "William Sankey\n",
    "</em>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "<p>\n",
    "Our Fall 2016 Incubator project was to see if we could identify party affiliation using Presidential candidates’ own words in conjunction with the latest Natural Language Processing (NLP) methods. NLP is the analysis of language used for communication by humans. Using python and the Natural Language Toolkit (NLTK) we were able to process political speech for classification purposes with roughly 80% accuracy.\n",
    "</p>\n",
    "<p>\n",
    "The origin of the word ‘shibboleth’ makes the case that language can be used to distinguish groups of individuals from one-another. To accomplish the task of identifying party affiliation through speech we needed language used by Republicans and Democrats, we set aside political independents for this analysis. The data most readily available for analysis, with identified Republicans and Democrats, was from presidential debates made publicly available through the The American Presidency Project (http://www.presidency.ucsb.edu/index.php). To account for shifting party lines and tone across time we focused on debates since the year 2000.\n",
    "</p>\n",
    "<p>\n",
    "The following provides snippets of the most critical parts of our code, for the full thing please see our repository here: LINK</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Munging</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data is publicly available there is a significant amount of processing needed to make it usuable for our NLP purposes. These tasks fall generally into getting the data and turning it into features that can be passed to a learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a function that will parse the data from the website\n",
    "def parse_website_b(url):\n",
    "    \"\"\"\n",
    "    Grabs data from website (url) and parses it.\n",
    "    Data format: in class_='displaytext', debate speaker is seperated by text with <b/> tag.\n",
    "    \n",
    "    Args:\n",
    "        url: the webpage to parse\n",
    "        \n",
    "    Returns:\n",
    "        json files saved to directory with debate info parsed by paragraph:\n",
    "        title, date, speaker, text\n",
    "        \n",
    "    \"\"\"\n",
    "    fetched = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(fetched, \"lxml\")\n",
    "\n",
    "    #Parsing\n",
    "    titles = unicode(soup.title.string)\n",
    "    dates = unicode(soup.find(\"span\", class_=\"docdate\").string)\n",
    "    body = soup.find(\"span\", class_=\"displaytext\")\n",
    "    paragraphs = soup.find(\"span\", class_=\"displaytext\").findChildren(\"p\")\n",
    "\n",
    "    #Creating a dataframe\n",
    "    text_list = []\n",
    "    speaker_list = []\n",
    "    child_list = []\n",
    "\n",
    "    #pull text and speaker from html\n",
    "    for paragraph in paragraphs:\n",
    "        text = unicode(paragraph.find(text=True, recursive=False))\n",
    "        text_list.append(text)\n",
    "        children = paragraph.findChildren('b')\n",
    "        for child in children:\n",
    "            child_list.append(child)\n",
    "        if child_list == []:\n",
    "            prevchild = body.find_previous_sibling('b')\n",
    "            speaker_list.append(prevchild)\n",
    "        else:\n",
    "            speakers = unicode(paragraph.b.get_text())\n",
    "            speaker_list.append(speakers)\n",
    "        child_list[:] = []\n",
    "\n",
    "    #replace 'None' in speaker list\n",
    "    start = next(element for element in speaker_list if element is not None)\n",
    "    for i, element in enumerate(speaker_list):\n",
    "        if element is None:\n",
    "            speaker_list[i] = start\n",
    "        else:\n",
    "            start = element\n",
    "\n",
    "    # Pandas dataframe\n",
    "    columns = {'text': text_list, 'speaker': speaker_list, 'title': titles, 'date': dates}\n",
    "    debates = pandas.DataFrame(columns)\n",
    "\n",
    "    # Exporting to JSON\n",
    "    directory_name = 'your/local/data/path'\n",
    "    base_filename = str(re.findall(r'\\d+', url))\n",
    "    suffix = '.json'\n",
    "    save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "\n",
    "    debates.to_json(save_path, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function takes a URL and returns the data in json format from the website. Importantly the data returned here is separated our by paragraph -- this means the data input into the classifier will ultimately come from paragraphs labelled with the part affiliation of the speaker. It also means that our classifier will ultimately take paragraphs as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two core principles in NLP are removing stop words (common words like 'the', 'and', 'it' etc.) and stemming words to find their root. For"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to tokenize and stem\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, removes stop words, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token if word not in stops]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
