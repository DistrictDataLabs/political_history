{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Shibboleth:\n",
    "NLP and the presidential debates</h1>\n",
    "<em>\n",
    "Jo Anna Capp\n",
    "Kevin O’Gallagher\n",
    "William Sankey\n",
    "</em>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "<p>\n",
    "Our Fall 2016 Incubator project sought to identify political party affiliation identification through speech. To accomplish this task we used Presidential candidate debates and the latest Natural Language Processing (NLP) methods. NLP is the analysis of language, from plain-spoken english of children to the polished rhetoric of politicans. Using python and the Natural Language Toolkit (NLTK) we were able to process political speech for classification purposes with roughly 80% accuracy. Essentially, given a paragraph of text we could estimate with 80% accuracy whether that paragraph leaned left or right on the American political spectrum.\n",
    "</p>\n",
    "<p>\n",
    "The data most readily available for analysis was from presidential debates made publicly available through the The American Presidency Project (http://www.presidency.ucsb.edu/index.php). This data is valuable in several ways: \n",
    "<ol>\n",
    "<li><strong>It is labelled:</strong>We know the candidate who made the response and their party affiliation. If we were to pull data from other venues, such as chatrooms or comment sections, we might infer party affiliation but we wouldn't <em>know</em> unless we asked that person...and even if we did ask we might not know how mainstream (in terms of the party represented) their opinion was.</li>\n",
    "<li><strong>It is relatively clean:</strong>These are prepared debates. The website hosting this data has clearly marked the speaker and organized their responses in structured paragraphs using clear HTML conventions. This makes data retrieval easy.</li>\n",
    "<li><strong>It can be verified:</strong>These debates were broadcast to millions of individuals. If there is some concern over the quality of our data at a future point in time, any critic can review our data against several independent sources to verify we have it right -- the candidates actually said these words.</li>\n",
    "</ol>\n",
    "As a final caveat, to account for shifting party lines and tone across time we focused on debates since the year 2000.\n",
    "</p>\n",
    "<p>\n",
    "The following provides snippets of the most critical parts of our code, for the full thing please see our repository here: https://github.com/DistrictDataLabs/political_history/branches</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Munging</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data is publicly available there is a significant amount of processing needed to make it usuable for our NLP purposes. These tasks fall generally into getting the data and turning it into features that can be passed to a learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a function that will parse the data from the website\n",
    "def parse_website_b(url):\n",
    "    \"\"\"\n",
    "    Grabs data from website (url) and parses it.\n",
    "    Data format: in class_='displaytext', debate speaker is seperated by text with <b/> tag.\n",
    "    \n",
    "    Args:\n",
    "        url: the webpage to parse\n",
    "        \n",
    "    Returns:\n",
    "        json files saved to directory with debate info parsed by paragraph:\n",
    "        title, date, speaker, text\n",
    "        \n",
    "    \"\"\"\n",
    "    fetched = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(fetched, \"lxml\")\n",
    "\n",
    "    #Parsing\n",
    "    titles = unicode(soup.title.string)\n",
    "    dates = unicode(soup.find(\"span\", class_=\"docdate\").string)\n",
    "    body = soup.find(\"span\", class_=\"displaytext\")\n",
    "    paragraphs = soup.find(\"span\", class_=\"displaytext\").findChildren(\"p\")\n",
    "\n",
    "    #Creating a dataframe\n",
    "    text_list = []\n",
    "    speaker_list = []\n",
    "    child_list = []\n",
    "\n",
    "    #pull text and speaker from html\n",
    "    for paragraph in paragraphs:\n",
    "        text = unicode(paragraph.find(text=True, recursive=False))\n",
    "        text_list.append(text)\n",
    "        children = paragraph.findChildren('b')\n",
    "        for child in children:\n",
    "            child_list.append(child)\n",
    "        if child_list == []:\n",
    "            prevchild = body.find_previous_sibling('b')\n",
    "            speaker_list.append(prevchild)\n",
    "        else:\n",
    "            speakers = unicode(paragraph.b.get_text())\n",
    "            speaker_list.append(speakers)\n",
    "        child_list[:] = []\n",
    "\n",
    "    #replace 'None' in speaker list\n",
    "    start = next(element for element in speaker_list if element is not None)\n",
    "    for i, element in enumerate(speaker_list):\n",
    "        if element is None:\n",
    "            speaker_list[i] = start\n",
    "        else:\n",
    "            start = element\n",
    "\n",
    "    # Pandas dataframe\n",
    "    columns = {'text': text_list, 'speaker': speaker_list, 'title': titles, 'date': dates}\n",
    "    debates = pandas.DataFrame(columns)\n",
    "\n",
    "    # Exporting to JSON\n",
    "    directory_name = 'your/local/data/path'\n",
    "    base_filename = str(re.findall(r'\\d+', url))\n",
    "    suffix = '.json'\n",
    "    save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "\n",
    "    debates.to_json(save_path, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function takes a URL and returns the website data in JSON format.\n",
    "\n",
    "Importantly the data returned here is separated our by paragraph -- this means the data input into the classifier will ultimately come from paragraphs labelled with the part affiliation of the speaker. It also means that our classifier will take paragraphs as input. At the end of this process we have a set of JSON files providing the part affiliation and text for each paragraph of every presidential debate since 2000.\n",
    "\n",
    "With the data sitting in a folder in our directory we can move on to cleaning it, a critical step ensuring the classifier has the right information it needs to learn on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two core principles in NLP are removing stop words (common words like 'the', 'and', 'it' etc.) and stemming words to find their root. In stemming words like 'thinking', 'acting', and 'approving' become 'think', 'act', and 'approv' respectively. Let's see how these tokenizers work on an exert of a paragraph from a 2000 debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to tokenize and stem\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, removes stop words, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token if word not in stops]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gore_text = \"We've got a bumper crop this year. But that's the good news.You know what the bad news is that follows on that. The prices are low. In the last several years, the so-called Freedom To Farm Law has, in my view, been mostly a failure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we got bumper crop year but good news you know bad news follow the price low in last sever year call freedom to farm law view most failur\n"
     ]
    }
   ],
   "source": [
    "print clean_text(gore_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process isn't always perfect but importantly it is isoldating the roots of the words in the paragraph and standardizing them for the learner.\n",
    "\n",
    "At the end of this cleaning process we have all the paragraphs from 2000 to the present looking very similar to the exert from Gore in 2000. This labelled information the classifier will use to learn on for our supervised learning exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing out models</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning starts with separating the data into training and testing samples (many also advocate for separating out a validation sample). A twenty percent test set is fairly standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ed3993af7968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train/test split of data (randomized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_validation' is not defined"
     ]
    }
   ],
   "source": [
    "#train/test split of data (randomized)\n",
    "text_train, text_test, labels_train, labels_test = cross_validation.train_test_split(text, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learner cannot take raw language as an input, we need to transform these paragraphs 1. into numpy arrays; and 2. into lists of words with some importance attached to each word for the classifier to learn. The `toarray()` method on the vectorizer transforms the paragraph into the numpy array while the TF-IDF process turns these lists of words into something we can use.\n",
    "\n",
    "Here's an aside on the TF-IDF process:\n",
    "\n",
    "<em>tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in information retrieval and text mining. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.</em>\n",
    "source: wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf vectorizer and numpy array\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "text_train_transformed = vectorizer.fit_transform(text_train).toarray()\n",
    "text_test_transformed  = vectorizer.transform(text_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on in the following lines of code but we are essentially finding the best features, fitting the model, and reviewing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build classifier pipeline\n",
    "select = SelectPercentile(f_classif)\n",
    "pca = PCA()\n",
    "feature_selection = FeatureUnion([('select', select), ('pca', pca)],\n",
    "                    transformer_weights={'pca': 10})\n",
    "clfNB = GaussianNB()\n",
    "\n",
    "steps1 = [('feature_selection', feature_selection),\n",
    "        ('naive_bayes', clf)]\n",
    "\n",
    "pipeline1 = sklearn.pipeline.Pipeline(steps1)\n",
    "\n",
    "#search for best parameters\n",
    "parameters1 = dict(feature_selection__select__percentile=[.05, .1, .25], \n",
    "              feature_selection__pca__n_components=[10, 50, 100])\n",
    "\n",
    "cv = sklearn.grid_search.GridSearchCV(pipeline1, param_grid=parameters1)\n",
    "\n",
    "#because tf-idf vectorizer isn't in this pipeline, fit/predict on transformed data\n",
    "cv.fit(text_train_transformed, labels_train)\n",
    "pred = cv.predict(text_test_transformed)\n",
    "\n",
    "print cv.best_params_\n",
    "\n",
    "#pipeline.fit(features_train, labels_train)\n",
    "#pred = pipeline.predict(features_test)\n",
    "report = sklearn.metrics.classification_report(labels_test, pred)\n",
    "print report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficacy of classifiers are measured in terms of accuracy, precision, recall, and area under the curve (auc) which shows the tradeoffs between False Positives and False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up scoring function and table\n",
    "scoring_table = PrettyTable(['pipeline_name', 'accuracy', 'precision', 'recall', 'auc'])\n",
    "\n",
    "def scoring_function(pipeline_name, test_labels, prediction):\n",
    "    \"\"\"\n",
    "    runs evaluation metrics on prediction from classifier\n",
    "    Args:\n",
    "        labels from the test data set, prediction from classifier     \n",
    "    Returns:\n",
    "        prints scoring functions, appends scores to scoring dataframe\n",
    "    \"\"\"\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, prediction)\n",
    "    precision = sklearn.metrics.precision_score(test_labels, prediction)\n",
    "    recall = sklearn.metrics.recall_score(test_labels, prediction)\n",
    "    auc = sklearn.metrics.roc_auc_score(test_labels, prediction)\n",
    "    print \"Validation Metrics for %s: accuracy: %s, precision: %s, recall: %s, auc: %s\"%(pipeline_name, accuracy, precision, recall, auc)\n",
    "    \n",
    "    scoring_table.add_row([pipeline_name, accuracy, precision, recall, auc])\n",
    "    return scoring_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines demonstrate how the essential components of a classifier are put togehter: from vectorizing the data, selecting the features, to specifying and fitting the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put together pieces of classifier\n",
    "\n",
    "#tf-idf vectorizer\n",
    "vectorizer1 = TfidfVectorizer(sublinear_tf=True)\n",
    "vectorizer2 = TfidfVectorizer(max_df = 1, min_df = 0, sublinear_tf=True)\n",
    "vectorizer3 = TfidfVectorizer(ngram_range = (1,3), sublinear_tf=True)\n",
    "vectorizer4 = TfidfVectorizer(max_df = 0.8, min_df = 0.2, ngram_range = (1,3), sublinear_tf=True)\n",
    "\n",
    "#feature selection\n",
    "select = SelectPercentile(f_classif)\n",
    "pca = PCA()\n",
    "feature_selection = FeatureUnion([('select', select), ('pca', pca)],\n",
    "                    transformer_weights={'pca': 10})\n",
    "\n",
    "#classifier\n",
    "clfNB = GaussianNB()\n",
    "clfAdaBoost = AdaBoostClassifier(random_state = 42)\n",
    "clfLR = LogisticRegression(random_state=42, solver='sag')\n",
    "clfSVM = SGDClassifier(loss='modified_huber', penalty='l2', n_iter=200, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested out a set of different models (18 in total and nearly all looked like the following lines of code). We choose the set of features that flow into the pipeline, we specify the type of classifier to use, and then we review the cross-validated results of the models. It is a tedious task that puts the 'science' in data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test2 - GaussianNB, simple vectorizer, PCA\n",
    "steps = [\n",
    "         ('feature_pick', pca),\n",
    "         ('classifier', clfNB)]\n",
    "\n",
    "params = dict(feature_pick__n_components=[100, 200, 500])\n",
    "\n",
    "prediction = gridsearch_pipeline('test2', text_train_transformed, labels_train, text_test_transformed, steps, params)\n",
    "scoring_function('test2', labels_test, prediction)\n",
    "print scoring_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Our final model showed a cross-validated 80% accuracy.\n",
    "\n",
    "MORE DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEE KEVIN'S REFERENCES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
