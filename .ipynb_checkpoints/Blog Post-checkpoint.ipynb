{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Shibboleth:\n",
    "NLP and the presidential debates</h1>\n",
    "<em>\n",
    "Jo Anna Capp\n",
    "Kevin O’Gallagher\n",
    "William Sankey\n",
    "</em>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "<p>\n",
    "Our Fall 2016 Incubator project was to see if we could identify party affiliation using Presidential candidates’ own words in conjunction with the latest Natural Language Processing (NLP) methods. NLP is the analysis of language used for communication by humans. Using python and the Natural Language Toolkit (NLTK) we were able to process political speech for classification purposes with roughly 80% accuracy.\n",
    "</p>\n",
    "<p>\n",
    "The origin of the word ‘shibboleth’ makes the case that language can be used to distinguish groups of individuals from one-another. To accomplish the task of identifying party affiliation through speech we needed language used by Republicans and Democrats, we set aside political independents for this analysis. The data most readily available for analysis, with identified Republicans and Democrats, was from presidential debates made publicly available through the The American Presidency Project (http://www.presidency.ucsb.edu/index.php). To account for shifting party lines and tone across time we focused on debates since the year 2000.\n",
    "</p>\n",
    "<p>\n",
    "The following provides snippets of the most critical parts of our code, for the full thing please see our repository here: LINK</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Munging</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the data is publicly available there is a significant amount of processing needed to make it usuable for our NLP purposes. These tasks fall generally into getting the data and turning it into features that can be passed to a learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a function that will parse the data from the website\n",
    "def parse_website_b(url):\n",
    "    \"\"\"\n",
    "    Grabs data from website (url) and parses it.\n",
    "    Data format: in class_='displaytext', debate speaker is seperated by text with <b/> tag.\n",
    "    \n",
    "    Args:\n",
    "        url: the webpage to parse\n",
    "        \n",
    "    Returns:\n",
    "        json files saved to directory with debate info parsed by paragraph:\n",
    "        title, date, speaker, text\n",
    "        \n",
    "    \"\"\"\n",
    "    fetched = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(fetched, \"lxml\")\n",
    "\n",
    "    #Parsing\n",
    "    titles = unicode(soup.title.string)\n",
    "    dates = unicode(soup.find(\"span\", class_=\"docdate\").string)\n",
    "    body = soup.find(\"span\", class_=\"displaytext\")\n",
    "    paragraphs = soup.find(\"span\", class_=\"displaytext\").findChildren(\"p\")\n",
    "\n",
    "    #Creating a dataframe\n",
    "    text_list = []\n",
    "    speaker_list = []\n",
    "    child_list = []\n",
    "\n",
    "    #pull text and speaker from html\n",
    "    for paragraph in paragraphs:\n",
    "        text = unicode(paragraph.find(text=True, recursive=False))\n",
    "        text_list.append(text)\n",
    "        children = paragraph.findChildren('b')\n",
    "        for child in children:\n",
    "            child_list.append(child)\n",
    "        if child_list == []:\n",
    "            prevchild = body.find_previous_sibling('b')\n",
    "            speaker_list.append(prevchild)\n",
    "        else:\n",
    "            speakers = unicode(paragraph.b.get_text())\n",
    "            speaker_list.append(speakers)\n",
    "        child_list[:] = []\n",
    "\n",
    "    #replace 'None' in speaker list\n",
    "    start = next(element for element in speaker_list if element is not None)\n",
    "    for i, element in enumerate(speaker_list):\n",
    "        if element is None:\n",
    "            speaker_list[i] = start\n",
    "        else:\n",
    "            start = element\n",
    "\n",
    "    # Pandas dataframe\n",
    "    columns = {'text': text_list, 'speaker': speaker_list, 'title': titles, 'date': dates}\n",
    "    debates = pandas.DataFrame(columns)\n",
    "\n",
    "    # Exporting to JSON\n",
    "    directory_name = 'your/local/data/path'\n",
    "    base_filename = str(re.findall(r'\\d+', url))\n",
    "    suffix = '.json'\n",
    "    save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "\n",
    "    debates.to_json(save_path, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function takes a URL and returns the data in json format from the website. Importantly the data returned here is separated our by paragraph -- this means the data input into the classifier will ultimately come from paragraphs labelled with the part affiliation of the speaker. It also means that our classifier will ultimately take paragraphs as input. At the end of this process we have a set of JSON files providing the part affiliation and text for each paragraph of every presidential debate since 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two core principles in NLP are removing stop words (common words like 'the', 'and', 'it' etc.) and stemming words to find their root. In stemming words like 'thinking', 'acting', and 'approving' become 'think', 'act', and 'approv' respectively (the process isn't always perfect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to tokenize and stem\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, removes stop words, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token if word not in stops]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing out models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train/test split of data (randomized)\n",
    "text_train, text_test, labels_train, labels_test = cross_validation.train_test_split(text, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf vectorizer and numpy array\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "text_train_transformed = vectorizer.fit_transform(text_train).toarray()\n",
    "text_test_transformed  = vectorizer.transform(text_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build classifier pipeline\n",
    "select = SelectPercentile(f_classif)\n",
    "pca = PCA()\n",
    "feature_selection = FeatureUnion([('select', select), ('pca', pca)],\n",
    "                    transformer_weights={'pca': 10})\n",
    "clfNB = GaussianNB()\n",
    "\n",
    "steps1 = [('feature_selection', feature_selection),\n",
    "        ('naive_bayes', clf)]\n",
    "\n",
    "pipeline1 = sklearn.pipeline.Pipeline(steps1)\n",
    "\n",
    "#search for best parameters\n",
    "parameters1 = dict(feature_selection__select__percentile=[.05, .1, .25], \n",
    "              feature_selection__pca__n_components=[10, 50, 100])\n",
    "\n",
    "cv = sklearn.grid_search.GridSearchCV(pipeline1, param_grid=parameters1)\n",
    "\n",
    "#because tf-idf vectorizer isn't in this pipeline, fit/predict on transformed data\n",
    "cv.fit(text_train_transformed, labels_train)\n",
    "pred = cv.predict(text_test_transformed)\n",
    "\n",
    "print cv.best_params_\n",
    "\n",
    "#pipeline.fit(features_train, labels_train)\n",
    "#pred = pipeline.predict(features_test)\n",
    "report = sklearn.metrics.classification_report(labels_test, pred)\n",
    "print report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up scoring function and table\n",
    "scoring_table = PrettyTable(['pipeline_name', 'accuracy', 'precision', 'recall', 'auc'])\n",
    "\n",
    "def scoring_function(pipeline_name, test_labels, prediction):\n",
    "    \"\"\"\n",
    "    runs evaluation metrics on prediction from classifier\n",
    "    Args:\n",
    "        labels from the test data set, prediction from classifier     \n",
    "    Returns:\n",
    "        prints scoring functions, appends scores to scoring dataframe\n",
    "    \"\"\"\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, prediction)\n",
    "    precision = sklearn.metrics.precision_score(test_labels, prediction)\n",
    "    recall = sklearn.metrics.recall_score(test_labels, prediction)\n",
    "    auc = sklearn.metrics.roc_auc_score(test_labels, prediction)\n",
    "    print \"Validation Metrics for %s: accuracy: %s, precision: %s, recall: %s, auc: %s\"%(pipeline_name, accuracy, precision, recall, auc)\n",
    "    \n",
    "    scoring_table.add_row([pipeline_name, accuracy, precision, recall, auc])\n",
    "    return scoring_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set-up generic grid-search cv function\n",
    "def gridsearch_pipeline(pipeline_name, train_data, train_labels, test_data, pipeline_steps, parameters):\n",
    "    \"\"\"\n",
    "    generic function to run gridsearchcv on an input dataset, pipeline, and parameters\n",
    "    Args:\n",
    "        data separated into features/labels and train/test\n",
    "        steps of the pipeline\n",
    "        parameters for gridsearchcv\n",
    "    Returns:\n",
    "        best parameters from gridsearch, prediction for test features\n",
    "    \"\"\"\n",
    "    #pipeline\n",
    "    pipe = sklearn.pipeline.Pipeline(pipeline_steps)\n",
    "    \n",
    "    #gridsearch\n",
    "    cv = sklearn.grid_search.GridSearchCV(pipe, param_grid=parameters)\n",
    "    cv.fit(train_data, train_labels)\n",
    "    pred = cv.predict(test_data)\n",
    "    print cv.best_params_\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put together pieces of classifier\n",
    "\n",
    "#tf-idf vectorizer\n",
    "vectorizer1 = TfidfVectorizer(sublinear_tf=True)\n",
    "vectorizer2 = TfidfVectorizer(max_df = 1, min_df = 0, sublinear_tf=True)\n",
    "vectorizer3 = TfidfVectorizer(ngram_range = (1,3), sublinear_tf=True)\n",
    "vectorizer4 = TfidfVectorizer(max_df = 0.8, min_df = 0.2, ngram_range = (1,3), sublinear_tf=True)\n",
    "\n",
    "#feature selection\n",
    "select = SelectPercentile(f_classif)\n",
    "pca = PCA()\n",
    "feature_selection = FeatureUnion([('select', select), ('pca', pca)],\n",
    "                    transformer_weights={'pca': 10})\n",
    "\n",
    "#classifier\n",
    "clfNB = GaussianNB()\n",
    "clfAdaBoost = AdaBoostClassifier(random_state = 42)\n",
    "clfLR = LogisticRegression(random_state=42, solver='sag')\n",
    "clfSVM = SGDClassifier(loss='modified_huber', penalty='l2', n_iter=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test2 - GaussianNB, simple vectorizer, PCA\n",
    "steps = [\n",
    "         ('feature_pick', pca),\n",
    "         ('classifier', clfNB)]\n",
    "\n",
    "params = dict(feature_pick__n_components=[100, 200, 500])\n",
    "\n",
    "prediction = gridsearch_pipeline('test2', text_train_transformed, labels_train, text_test_transformed, steps, params)\n",
    "scoring_function('test2', labels_test, prediction)\n",
    "print scoring_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Final Model and Discussion</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
