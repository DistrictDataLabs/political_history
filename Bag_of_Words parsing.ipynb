{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### munging/parsing/processing json data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ideas:  \n",
    "use date of debate as another feature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "LOCAL_DATA_PATH = 'C:\\Users\\JoAnna\\political_history\\data'\n",
    "LOCAL_SAVE_PATH = 'C:\\Users\\JoAnna\\political_history\\processed_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general processing for all approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import all json files, concatenate into pandas dataframe\n",
    "all_files = glob.glob(LOCAL_DATA_PATH + '/*.json')\n",
    "\n",
    "df = pd.concat((pd.read_json(f, orient='index') for f in all_files))\n",
    "\n",
    "#concatenating resulted in non-unique index, re-index\n",
    "#df.index.is_unique\n",
    "\n",
    "df['index'] = np.arange(len(df))\n",
    "df = df.set_index('index')\n",
    "\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from speaker: strip spaces and special characters, make lowercase\n",
    "df.speaker = df.speaker.str.strip().str.lower().str.replace(' ','').str.replace('.', '').str.replace(':','')\n",
    "\n",
    "#get list of unique values for speaker\n",
    "unique_speaker = pd.unique(df.speaker.ravel())\n",
    "#print len(unique_speaker)\n",
    "#print unique_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Map list of candidates to political party\n",
    "speaker_to_party = {'trump': 'Republican',\n",
    "                    'clinton': 'Democrat',\n",
    "                    'pence': 'Republican',\n",
    "                    'kaine': 'Democrat',\n",
    "                    'republicanpresidentialnomineewmittromney': 'Republican',\n",
    "                    'govromney': 'Republican',\n",
    "                    'thepresident\\u2014': 'Republican',\n",
    "                    'govromney\\u2014': 'Republican',\n",
    "                    'govromney\\u2014\\u2014': 'Republican',\n",
    "                    'thepresident': 'Democrat',\n",
    "                    'representativepaulryan': 'Republican',\n",
    "                    'ryan': 'Republican',\n",
    "                    'vicepresidentjosephbiden': 'Democrat',\n",
    "                    'biden': 'Democrat',\n",
    "                    'mccain': 'Republican',\n",
    "                    'obama': 'Democrat',\n",
    "                    'palin': 'Republican',\n",
    "                    'presidentbush': 'Republican',\n",
    "                    'senatorjohnfkerry': 'Democrat',\n",
    "                    'senatorkerry': 'Democrat',\n",
    "                    'cheney': 'Republican',\n",
    "                    'edwards': 'Democrat',\n",
    "                    'bush': 'Republican',\n",
    "                    'gore': 'Democrat',\n",
    "                    'lieberman': 'Democrat'}\n",
    "\n",
    "#make new column in dataframe for affiliation\n",
    "df['affiliation'] = df['speaker']\n",
    "df['affiliation'].replace(speaker_to_party, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create two new dataframes, one for republicans, one for democrats\n",
    "republican_df = df.loc[df['affiliation'] == 'Republican']\n",
    "democrat_df = df.loc[df['affiliation'] == 'Democrat']\n",
    "\n",
    "#create combined data frame - better for train/test split (sort of...)\n",
    "candidates_df = df.loc[df['affiliation'].isin(['Republican','Democrat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export new dataframe for others to use\n",
    "directory_name = LOCAL_SAVE_PATH\n",
    "base_filename = 'candidates'\n",
    "suffix = '.pkl'\n",
    "save_path = os.path.join(directory_name, base_filename + suffix)\n",
    "candidates_df.to_pickle(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read pickled data\n",
    "#test_df = pd.read_pickle('C:\\Users\\JoAnna\\political_history\\processed_data\\candidates.pkl')\n",
    "#print test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoAnna\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.py:3443: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "#label data - 0 for democrat, 1 for republican\n",
    "candidates_df['affiliation'].replace({'Democrat':0, 'Republican':1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### processing the text column for bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make new lists just with labels and text\n",
    "labels = candidates_df['affiliation']\n",
    "paragraph_text = candidates_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define function to tokenize and stem\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, removes stop words, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token if word not in stops]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) \n",
    "\n",
    "num_paragraphs= len(paragraph_text)\n",
    "cleaned_paragraphs = []\n",
    "#loop over paragraph_text to clean\n",
    "for paragraph in paragraph_text:\n",
    "    cleaned_text = clean_text(paragraph)\n",
    "    cleaned_paragraphs.append(cleaned_text)\n",
    "\n",
    "#print len(cleaned_paragraphs)\n",
    "#print cleaned_paragraphs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a mapping of words to stemmed words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#export labels and cleaned paragraphs\n",
    "os.chdir(LOCAL_SAVE_PATH)\n",
    "pickle.dump(labels, open(\"bow_labels.pkl\", \"w\"))\n",
    "pickle.dump(cleaned_paragraphs, open(\"bow_processed_text.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define new function to tokenize and stem, keep stopwords\n",
    "def clean_text_nostop(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, converts all characters to lowercase, stems\n",
    "    \n",
    "    Args:\n",
    "        a single string of text \n",
    "        \n",
    "    Returns:\n",
    "        processed text string\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = RegexpTokenizer(r'\\w+')\n",
    "    #stops = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    token = tokens.tokenize(text)\n",
    "    filtered_words = [word for word in token]\n",
    "    stems = [stemmer.stem(t) for t in filtered_words]\n",
    "    return( \" \".join(stems)) \n",
    "\n",
    "num_paragraphs= len(paragraph_text)\n",
    "cleaned_paragraphs_nostop = []\n",
    "#loop over paragraph_text to clean\n",
    "for paragraph in paragraph_text:\n",
    "    cleaned_text_nostop = clean_text_nostop(paragraph)\n",
    "    cleaned_paragraphs_nostop.append(cleaned_text_nostop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export labels and cleaned paragraphs, no stop words\n",
    "os.chdir(LOCAL_SAVE_PATH)\n",
    "#pickle.dump(labels, open(\"bow_labels.pkl\", \"w\"))\n",
    "pickle.dump(cleaned_paragraphs_nostop, open(\"bow_processed_text_nostop.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paragraph processing for word2vec\n",
    "#https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "\n",
    "import nltk.data\n",
    "nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Define a function to split a review into parsed sentences\n",
    "def paragraph_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
